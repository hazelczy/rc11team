{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "75618c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import quote  # 确保引入了quote函数\n",
    "import csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "34dcbd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from urllib.parse import quote\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d6e1b66a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "搜索 Subtitle 时出现错误：'NoneType' object has no attribute 'find'\n",
      "搜索 Christ and the Samaritan Woman 时出现错误：'NoneType' object has no attribute 'find'\n",
      "Query: Portrait of Giovanna degli Albizzi Tornabuoni\n",
      "URL: https://en.wikipedia.org/wiki/Portrait_of_Giovanna_Tornabuoni\n",
      "First Paragraph: The Portrait of Giovanna Tornabuoni (also known as Portrait of Giovanna degli Albizzi[1]) is a painting by the Italian Renaissance painter Domenico Ghirlandaio, executed in 1488 and located in the Museo Thyssen-Bornemisza, Madrid. The portrait was commissioned by Lorenzo Tornabuoni after his wife's death in 1488 and includes many symbolic details.[2]\n",
      "\n",
      "搜索 Jesus Among the Doctors 时出现错误：'NoneType' object has no attribute 'find'\n",
      "搜索 Young Knight in a Landscape 时出现错误：'NoneType' object has no attribute 'find'\n",
      "Query: The Nymph at the Fountain\n",
      "URL: https://en.wikipedia.org/wiki/Nymph\n",
      "First Paragraph: A nymph (Ancient Greek: νύμφη, romanized: nýmphē, Modern Greek: nímfi; Attic Greek: [nýmpʰɛː], Modern Greek: [ˈniɱfi]), sometimes spelled nymphe, in ancient Greek folklore is a minor female nature deity. Different from other Greek goddesses, nymphs are generally regarded as personifications of nature, are typically tied to a specific place or landform, and are usually depicted as maidens. They were immortal like other goddesses, except for the Hamadryads, whose lives were bound to a specific tree.[1]\n",
      "\n",
      "Query: The Grand Canal from San Vio, Venice\n",
      "URL: https://en.wikipedia.org/wiki/Grand_Canal_(Venice)_architecture\n",
      "First Paragraph: The Grand Canal (Italian: Canal Grande [kaˌnal ˈɡrande]; Venetian: Canal Grando, anciently Canałasso [kanaˈɰaso]) is the central water course in the city of Venice, Italy.\n",
      "\n",
      "搜索 The Annunciation 时出现错误：'NoneType' object has no attribute 'find'\n",
      "搜索 Saint Catherine of Alexandria 时出现错误：'NoneType' object has no attribute 'find'\n",
      "搜索 Venus and Cupid 时出现错误：'NoneType' object has no attribute 'find'\n",
      "搜索 Family Group in a Landscape 时出现错误：'NoneType' object has no attribute 'find'\n",
      "搜索 The See-Saw 时出现错误：'NoneType' object has no attribute 'find'\n",
      "Query: Les Vessenots in Auvers\n",
      "URL: https://en.wikipedia.org/wiki/Auvers-sur-Oise\n",
      "First Paragraph: Auvers-sur-Oise (French pronunciation: [ovɛʁ syʁ waz] ⓘ, lit. \"Auvers-on-Oise\") is a commune in the department of Val-d'Oise, on the northwestern outskirts of Paris, France. It is located 27.2 km (16.9 mi) from the centre of Paris. It is associated with several famous artists, the most prominent being Vincent van Gogh (1853–1890). This was the place where Van Gogh died.\n",
      "\n",
      "搜索 Waterloo Bridge 时出现错误：'NoneType' object has no attribute 'find'\n",
      "搜索 The Dream 时出现错误：'NoneType' object has no attribute 'find'\n",
      "搜索 Metropolis 时出现错误：'NoneType' object has no attribute 'find'\n",
      "搜索 Hotel Room 时出现错误：'NoneType' object has no attribute 'find'\n",
      "搜索 Quappi in Pink Jumper 时出现错误：'NoneType' object has no attribute 'find'\n",
      "Query: Dream Caused by the Flight of a Bee around a Pomegranate a Second before Waking\n",
      "URL: https://en.wikipedia.org/wiki/Dream_Caused_by_the_Flight_of_a_Bee_Around_a_Pomegranate_a_Second_Before_Awakening\n",
      "First Paragraph: Dream Caused by the Flight of a Bee Around a Pomegranate a Second Before Awakening is a surrealist painting by Salvador Dalí. A shorter alternate title for the painting is Dream Caused by the Flight of a Bee. It was painted in 1944, and the woman in the painting, dreaming, is said to represent his wife, Gala.[1][2] The painting is currently in the Thyssen-Bornemisza Museum, in Madrid.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def search_wikipedia(query):\n",
    "    # 构建Wikipedia搜索URL\n",
    "    search_url = f\"https://en.wikipedia.org/w/index.php?search={query.replace(' ', '+')}\"\n",
    "    try:\n",
    "        response = requests.get(search_url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # 获取搜索结果中的第一个链接\n",
    "        first_link = soup.find('div', class_='mw-search-result-heading').find('a')['href']\n",
    "        return \"https://en.wikipedia.org\" + first_link\n",
    "    except Exception as e:\n",
    "        print(f\"搜索 {query} 时出现错误：{e}\")\n",
    "        return None\n",
    "\n",
    "def extract_first_paragraph(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        first_paragraph = soup.find('p').text.strip()\n",
    "        return first_paragraph\n",
    "    except Exception as e:\n",
    "        print(f\"从 {url} 提取时出现错误：{e}\")\n",
    "        return None\n",
    "\n",
    "def process_csv(file_path):\n",
    "    with open(file_path, newline='', encoding='utf-8') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        for row in reader:\n",
    "            query = row[0]  # 假设关键词在每行的第一列\n",
    "            search_result_url = search_wikipedia(query)\n",
    "            if search_result_url:\n",
    "                first_paragraph = extract_first_paragraph(search_result_url)\n",
    "                if first_paragraph:\n",
    "                    print(f\"Query: {query}\\nURL: {search_result_url}\\nFirst Paragraph: {first_paragraph}\\n\")\n",
    "\n",
    "csv_file_path = 'C:\\\\Users\\\\ASUS\\\\Desktop\\\\t2 rc11\\\\subtitles.csv'\n",
    "process_csv(csv_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4611440d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "have something wrong with Subtitle ：'NoneType' object has no attribute 'find'\n",
      "have something wrong with Christ and the Samaritan Woman ：'NoneType' object has no attribute 'find'\n",
      "have something wrong with Jesus Among the Doctors ：'NoneType' object has no attribute 'find'\n",
      "have something wrong with Young Knight in a Landscape ：'NoneType' object has no attribute 'find'\n",
      "have something wrong with The Annunciation ：'NoneType' object has no attribute 'find'\n",
      "have something wrong with Saint Catherine of Alexandria ：'NoneType' object has no attribute 'find'\n",
      "have something wrong with Venus and Cupid ：'NoneType' object has no attribute 'find'\n",
      "have something wrong with Family Group in a Landscape ：'NoneType' object has no attribute 'find'\n",
      "have something wrong with The See-Saw ：'NoneType' object has no attribute 'find'\n",
      "have something wrong with Waterloo Bridge ：'NoneType' object has no attribute 'find'\n",
      "have something wrong with The Dream ：'NoneType' object has no attribute 'find'\n",
      "have something wrong with Metropolis ：'NoneType' object has no attribute 'find'\n",
      "have something wrong with Hotel Room ：'NoneType' object has no attribute 'find'\n",
      "have something wrong with Quappi in Pink Jumper ：'NoneType' object has no attribute 'find'\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def search_wikipedia(query):\n",
    "    # 构建Wikipedia搜索URL\n",
    "    search_url = f\"https://en.wikipedia.org/w/index.php?search={query.replace(' ', '+')}\"\n",
    "    try:\n",
    "        response = requests.get(search_url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # 获取搜索结果中的第一个链接\n",
    "        first_link = soup.find('div', class_='mw-search-result-heading').find('a')['href']\n",
    "        return \"https://en.wikipedia.org\" + first_link\n",
    "    except Exception as e:\n",
    "        print(f\"have something wrong with {query} ：{e}\")\n",
    "        return None\n",
    "\n",
    "def extract_first_paragraph(url):\n",
    "    if not url:\n",
    "        return \"no result\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        first_paragraph = soup.find('p').text.strip()\n",
    "        return first_paragraph\n",
    "    except Exception as e:\n",
    "        print(f\"have something wrong with{url} ：{e}\")\n",
    "        return \"no result\"\n",
    "\n",
    "def process_csv(file_path, output_file_path):\n",
    "    results = []\n",
    "    with open(file_path, newline='', encoding='utf-8') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        for row in reader:\n",
    "            query = row[0]  # 假设关键词在每行的第一列\n",
    "            search_result_url = search_wikipedia(query)\n",
    "            first_paragraph = extract_first_paragraph(search_result_url)\n",
    "            results.append([query, search_result_url, first_paragraph])\n",
    "\n",
    "    # 将结果写入新的CSV文件\n",
    "    with open(output_file_path, mode='w', newline='', encoding='utf-8') as output_file:\n",
    "        writer = csv.writer(output_file)\n",
    "        writer.writerow(['Query', 'URL', 'First Paragraph'])\n",
    "        for result in results:\n",
    "            writer.writerow(result)\n",
    "\n",
    "csv_file_path = 'C:\\\\Users\\\\ASUS\\\\Desktop\\\\t2 rc11\\\\subtitles.csv'\n",
    "output_csv_path = 'C:\\\\Users\\\\ASUS\\\\Desktop\\\\t2 rc11\\\\outputsubtitles.csv'\n",
    "process_csv(csv_file_path, output_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fbad55da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import quote\n",
    "\n",
    "def search_wikipedia(query):\n",
    "    search_url = f\"https://en.wikipedia.org/w/index.php?search={quote(query)}\"\n",
    "    response = requests.get(search_url)\n",
    "    if response.history:  # 检查是否有重定向\n",
    "        final_url = response.url  # 获取重定向后的URL\n",
    "    else:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        first_link_element = soup.find('div', class_='mw-search-result-heading')\n",
    "        if first_link_element:\n",
    "            first_link = first_link_element.find('a')['href']\n",
    "            final_url = \"https://en.wikipedia.org\" + first_link\n",
    "        else:\n",
    "            final_url = None\n",
    "    return final_url\n",
    "\n",
    "def process_csv(file_path, output_file_path):\n",
    "    results = []\n",
    "    try:\n",
    "        # 尝试使用不同的编码读取文件\n",
    "        with open(file_path, newline='', encoding='utf-8') as csvfile:\n",
    "            reader = csv.reader(csvfile)\n",
    "            for row in reader:\n",
    "                process_row(row, results)\n",
    "    except UnicodeDecodeError:\n",
    "        # 如果遇到编码错误，则使用ISO-8859-1编码再次尝试\n",
    "        with open(file_path, newline='', encoding='ISO-8859-1') as csvfile:\n",
    "            reader = csv.reader(csvfile)\n",
    "            for row in reader:\n",
    "                process_row(row, results)\n",
    "\n",
    "    # Write results to a new CSV file\n",
    "    with open(output_file_path, mode='w', newline='', encoding='utf-8') as output_file:\n",
    "        writer = csv.writer(output_file)\n",
    "        writer.writerow(['Item Name', 'Item URL', 'Item First Paragraph', 'Artist Name', 'Artist URL', 'Artist First Paragraph'])\n",
    "        for result in results:\n",
    "            writer.writerow(result)\n",
    "\n",
    "def process_item_info(row, results):\n",
    "    # 假设藏品名在第二列\n",
    "    item_name = row[1]\n",
    "    item_search_result_url = search_wikipedia(item_name)\n",
    "    item_first_paragraph = extract_first_paragraph(item_search_result_url)\n",
    "    # 将结果保存到results列表中\n",
    "    results.append([item_name, item_search_result_url, item_first_paragraph])\n",
    "\n",
    "def process_artist_info(row, results):\n",
    "    # 假设艺术家名在第三列\n",
    "    artist_name = row[2]\n",
    "    artist_search_result_url = search_wikipedia(artist_name)\n",
    "    artist_first_paragraph = extract_first_paragraph(artist_search_result_url)\n",
    "    # 将结果保存到results列表中\n",
    "    results.append([artist_name, artist_search_result_url, artist_first_paragraph])\n",
    "\n",
    "# Make sure to define search_wikipedia and extract_first_paragraph functions as well\n",
    "\n",
    "csv_file_path = 'C:\\\\Users\\\\ASUS\\\\Desktop\\\\t2 rc11\\\\final csv\\\\thyssen.csv'\n",
    "output_csv_path = 'C:\\\\Users\\\\ASUS\\\\Desktop\\\\t2 rc11\\\\final csv\\\\thyssen_output_csv_file.csv'\n",
    "process_csv(csv_file_path, output_csv_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "47434701",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import quote\n",
    "\n",
    "def search_wikipedia(query):\n",
    "    search_url = f\"https://en.wikipedia.org/w/index.php?search={quote(query)}\"\n",
    "    response = requests.get(search_url)\n",
    "    if response.history:  # 检查是否有重定向\n",
    "        final_url = response.url  # 获取重定向后的URL\n",
    "    else:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        first_link_element = soup.find('div', class_='mw-search-result-heading')\n",
    "        if first_link_element:\n",
    "            first_link = first_link_element.find('a')['href']\n",
    "            final_url = \"https://en.wikipedia.org\" + first_link\n",
    "        else:\n",
    "            final_url = None\n",
    "    return final_url\n",
    "\n",
    "def process_csv(file_path, output_file_path):\n",
    "    results = []\n",
    "    try:\n",
    "        # 尝试使用不同的编码读取文件\n",
    "        with open(file_path, newline='', encoding='utf-8') as csvfile:\n",
    "            reader = csv.reader(csvfile)\n",
    "            for row in reader:\n",
    "                process_row(row, results)\n",
    "    except UnicodeDecodeError:\n",
    "        # 如果遇到编码错误，则使用ISO-8859-1编码再次尝试\n",
    "        with open(file_path, newline='', encoding='ISO-8859-1') as csvfile:\n",
    "            reader = csv.reader(csvfile)\n",
    "            for row in reader:\n",
    "                process_row(row, results)\n",
    "\n",
    "    # Write results to a new CSV file\n",
    "    with open(output_file_path, mode='w', newline='', encoding='utf-8') as output_file:\n",
    "        writer = csv.writer(output_file)\n",
    "        writer.writerow(['Item Name', 'Item URL', 'Item First Paragraph', 'Artist Name', 'Artist URL', 'Artist First Paragraph'])\n",
    "        for result in results:\n",
    "            writer.writerow(result)\n",
    "\n",
    "def process_item_info(row, results):\n",
    "    # 假设藏品名在第二列\n",
    "    item_name = row[1]\n",
    "    item_search_result_url = search_wikipedia(item_name)\n",
    "    item_first_paragraph = extract_first_paragraph(item_search_result_url)\n",
    "    # 将结果保存到results列表中\n",
    "    results.append([item_name, item_search_result_url, item_first_paragraph])\n",
    "\n",
    "def process_artist_info(row, results):\n",
    "    # 假设艺术家名在第三列\n",
    "    artist_name = row[2]\n",
    "    artist_search_result_url = search_wikipedia(artist_name)\n",
    "    artist_first_paragraph = extract_first_paragraph(artist_search_result_url)\n",
    "    # 将结果保存到results列表中\n",
    "    results.append([artist_name, artist_search_result_url, artist_first_paragraph])\n",
    "\n",
    "# Make sure to define search_wikipedia and extract_first_paragraph functions as well\n",
    "\n",
    "csv_file_path = 'C:\\\\Users\\\\ASUS\\\\Desktop\\\\t2 rc11\\\\final csv\\\\the national gallery.csv'\n",
    "output_csv_path = 'C:\\\\Users\\\\ASUS\\\\Desktop\\\\t2 rc11\\\\final csv\\\\tng_output_csv_file.csv'\n",
    "process_csv(csv_file_path, output_csv_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ee5fcd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
